{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![River-logo](../assets/river_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General introduction to River and its functionality\n",
    "\n",
    "`River` is a Python library for online machine learning. It first appears on November 2020, which is a result of a merge between [`creme`](https://github.com/MaxHalford/creme) and [`scikit-multiflow`](https://github.com/scikit-multiflow/scikit-multiflow).\n",
    "\n",
    "Currently, with more than 3000 stars on `Github` and itself being applied in various data science project at major tech firms (Samsung/Harman Kardon, Bee2Beep, BNP Paribas, Lyft, etc.), `River` has turned itself to become a go-to library for machine learning on streaming data.\n",
    "\n",
    "Key features of `River`:\n",
    "\n",
    "* **Incremental learning**: For all models within `River`, only one sample is updated at a time.\n",
    "* **Adaptive learning**: Methods are robust against any concept drift/anomalies in dynamic situations.\n",
    "* **General**: `River` is designed for all-purpose use, just like `scikit-learn`. Models within the package deals with all kinds of problems, including regression, classification, clustering, and even ad-hoc tasks like pre-processing, etc.\n",
    "* **Efficient**: Streaming methods in `River` are designed to consume time and memory at an efficient manner, mostly sub-linear.\n",
    "* **Easy to use**: `River` is suitable for use at any experience level, from beginners to experts/researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From batch learning to stream learning\n",
    "\n",
    "## Batch learning\n",
    "\n",
    "Batch learning assumes that **all** data is available at the same time, for both training and testing, and is processed in batches or as a whole.\n",
    "\n",
    "A minimal pipeline for traditional machine learning includes:\n",
    "\n",
    "1. Data preprocessing and engineering\n",
    "1. Training\n",
    "1. Testing\n",
    "\n",
    "We will take an example with [`Iris`](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), a dataset that everyone in the world of machine learning should all have heard of, at least once. This is available as part of `scikit-learn`.\n",
    "\n",
    "With this dataset, we will use $75\\%$ of them for training, and the remaining $25\\%$ for testing. The training and testing set have to be totally separated from each other, i.e they cannot overlap on one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as datasets_skl\n",
    "from sklearn.naive_bayes import GaussianNB as GaussianNB_skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets_skl.load_iris()\n",
    "\n",
    "# load variables and output into different variables\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split into training and testing set\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size = 0.25, stratify=y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shape of datasets\n",
    "print(f\"Shape of iris: {iris.data.shape}, \"\n",
    "      f\"training set: {x_train.shape}, testing set: {x_test.shape}\")\n",
    "\n",
    "# feature names\n",
    "print(f\"The iris feature names include:\", \n",
    "      ', '.join(str(p) for p in iris.feature_names))\n",
    "\n",
    "# target names\n",
    "print(f\"The iris target names (groups) include:\", \n",
    "      ', '.join(str(p) for p in iris.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem, with $4$ features and $3$ groups (target names). As such, for this problem, we will use the Gaussian Naive Bayes (`GaussianNB`) for a parallel comparision against its incremental version in `River`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussiannb_sklearn = GaussianNB_skl()\n",
    "gaussiannb_sklearn.fit(X=x_train, y=y_train)\n",
    "y_pred = gaussiannb_sklearn.predict(X=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive accuracy\n",
    "print(f\"SKLearn's Gaussian Naive Bayes accuracy: {accuracy_score(y_test, y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model accuracy of $97\\%$, given that the dataset is simple and has a small number of data points, is indeed acceptable. However, in real life scenarios, generally speaking, we can not have all the data available at the same time. In fact, they usually come at a constant rate, which means that we have to collect data over time and generate new models after a certain time window.\n",
    "\n",
    "However, this approach raises a lot of relevant questions, include\n",
    "\n",
    "* Should we train new models on new data or even old data? To what time point?\n",
    "* How much new data is enough to start creating a new model?\n",
    "* How much should the new model be better than the old one, in terms of cost effectiveness?\n",
    "* Storing data (especially for big data problems) is sometimes not feasible, impratical and/or too costly\n",
    "* etc.\n",
    "\n",
    "All of these problems, combined, triggered the formation of stream learning practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the assumtion that\n",
    "\n",
    "* Data comes in a stream, which means that we do not have all the data available at the same time;\n",
    "* The number of data points are infinite; and \n",
    "* The statistics of the future-coming points are not predictable.\n",
    "\n",
    "The stream learning methods are specifically designed to deal with this situation under a certain set of requirements:\n",
    "\n",
    "* **Data are processed at one sample at a time.**: A sample is only seen once (learned once) and then the information regarding this point is totally forgotten after processing.\n",
    "* **Time and memory efficient**: Although the data stream is infinite, the algorithm must be designed to be efficient enough to handle a large number of data points coming in a continuous pattern. Most online learning algorithms deemed to use resources at a sub-linear fashion.\n",
    "* **Able to predict at any time**: The model can provide predictions for any data point coming at any time.\n",
    "\n",
    "Within the scope of this demo, the stream can be simulated by iterating through the whole dataset, *one observation at a time* with `zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_i, y_i in zip(x_train, y_train):\n",
    "    # TBA\n",
    "    pass\n",
    "x_i, y_i # final observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with a large number of data in 2D, `numpy.ndarrays` are much more preferred. Most machine learning packages in Python, including `scikit-learn`, natively deals with this type of data. \n",
    "\n",
    "However, when dealing with data coming in a streaming fashion, particularly under the assumption that data only comes one point at a time as one-dimensional arrays, dictionary is much more preferred. As such, when feeding the data into `River`'s algorithms, we need a bit of transformation. This can easily be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_i, y_i in zip(x_train, y_train):\n",
    "    x_i_dict = dict(zip(iris.feature_names, x_i))\n",
    "\n",
    "x_i_dict, y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although `scikit-learn` supports certain incremental algorithms (i.e these methods are provided with a `partial_fit` methods), this method only works for  *mini batches* - data that comes in at small batches at the same time.  \n",
    "\n",
    "This also means that, we are doing \"training\", and then \"testing\" later, which is not the suitable practice for fully online machine learing.\n",
    "\n",
    "In the following example with `River`, we are also using the Gaussian Naive Bayes method, which is an **incremental method** itself. The training and predicting method are provided via `learn_one` and `predict_one`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `progressive_val_score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from river import evaluate\n",
    "from river import stream\n",
    "from river import naive_bayes\n",
    "from river import metrics\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `progressive_val_score` method implements the **prequential evaluation** and requires three components: \n",
    "\n",
    "- a data stream\n",
    "- a model\n",
    "- a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Setup stream and estimators\n",
    "data = stream.iter_sklearn_dataset(datasets_skl.load_iris(), \n",
    "                                   shuffle=True, \n",
    "                                   seed=42)\n",
    "nb = naive_bayes.GaussianNB()\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "# Setup evaluator\n",
    "evaluate.progressive_val_score(dataset=data, \n",
    "                               model=nb, \n",
    "                               metric=metric, \n",
    "                               print_every=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood\n",
    "\n",
    "In the streaming setting, training and predicting happen inside a loop. Remember, a data sample is available only once in the stream.\n",
    "\n",
    "The `iter_sklearn` method converts a `scikit-learn` dataset object into an iterable that returns the data as `dict`. This way, we do not need the manual `zip` command as mentioned previously.\n",
    "\n",
    "The following example shows the main steps performed by `progressive_val_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = naive_bayes.GaussianNB()\n",
    "metric = metrics.Accuracy()\n",
    "data = stream.iter_sklearn_dataset(load_iris(), shuffle=True, seed=42)\n",
    "n_passed_samples = 0\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for x, y in data:\n",
    "    # Predict class\n",
    "    y_p = model.predict_one(x)                    \n",
    "    if y_p is not None:\n",
    "        # Update metric\n",
    "        metric.update(y_true=y, y_pred=y_p)       \n",
    "    # Train the model\n",
    "    model.learn_one(x, y)  \n",
    "    # Process next sample if the model is empty\n",
    "    if y_p is None:\n",
    "        continue\n",
    "    # Add one to the number of passed samples\n",
    "    n_passed_samples += 1\n",
    "    y_pred.append(y_p)\n",
    "    y_true.append(y)\n",
    "    if n_passed_samples % 15 == 0:\n",
    "        print(f'[{metric.cm.n_samples + 1}] {metric}')\n",
    "        \n",
    "print(f'Model accuracy calculated by accuracy_score from sklearn: {accuracy_score(y_true, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The order matters!\n",
    "\n",
    "Notice the *order* of the training and prediction operations.\n",
    "\n",
    "When a new data sample ($x_i$, $y_i$) arrives:\n",
    "\n",
    "1. Predict the label $y_i'$ for the new data sample $x_i$, *not yet \"seen\"* by the model.\n",
    "2. Train the model using the new sample ($x_i$, $y_i$).\n",
    "\n",
    "This practice is commonly known as **prequential evaluation** or **interleaved test-then-train** evaluation. This means that, the method is used to predict the outcome of the new data point, even before learning.\n",
    "\n",
    "A benefit from this approach is that we can use all labeled samples in the stream to measure a model's performance, since the model can provide predictions at any point in the stream, depending on its current internal state. Batch methods can only predict after they have finished training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = naive_bayes.GaussianNB()\n",
    "metric = metrics.Accuracy()\n",
    "data = stream.iter_sklearn_dataset(load_iris(), shuffle=True, seed=0)\n",
    "n_passed_samples = 0\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for x, y in data:\n",
    "    # Predict class\n",
    "    y_p = model.predict_one(x)                    \n",
    "    if y_p is not None:\n",
    "        # Update metric\n",
    "        metric.update(y_true=y, y_pred=y_p)       \n",
    "    # Train the model\n",
    "    model.learn_one(x, y)  \n",
    "    # Process next sample if the model is empty\n",
    "    if y_p is None:\n",
    "        continue\n",
    "    # Add one to the number of passed samples\n",
    "    n_passed_samples += 1\n",
    "    y_pred.append(y_p)\n",
    "    y_true.append(y)\n",
    "    if n_passed_samples % 15 == 0:\n",
    "        print(f'[{metric.cm.n_samples + 1}] {metric}')\n",
    "        \n",
    "print(f'Model accuracy calculated by accuracy_score from sklearn: {accuracy_score(y_true, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key facts\n",
    "\n",
    "- We use *all* labeled samples in the stream to measure a model's performance. \n",
    "- The model provides predictions at any point in the stream, according to its *current* internal state. Batch methods can only predict after they finished training.\n",
    "- All methods (models, metrics, etc.) in `River` are updated **one sample at a time**\n",
    "  - Mini-batch processing is supported by some methods.\n",
    "\n",
    "### Dictionaries\n",
    "\n",
    "The default data structure used internally in `River`.\n",
    "\n",
    "- Efficient storage of 1-dimensional data (1 sample)\n",
    "- Supports sparse/missing data\n",
    "- Easy access to features by name (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last sample from the experiment above\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides methods, performance metrics in `River` can also be updated with one sample at a time. This means that we do not need to store all true and predicted labels in two separate arrays, which is sometimes *impractical*. \n",
    "\n",
    "Moreover, these metrics share the same  **confusion matrix**, which is efficient for calculating various metrics at the same time, for one single data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.metrics import Accuracy\n",
    "\n",
    "metric = Accuracy()\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    metric.update(y_true=yt, y_pred=yp)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix from all data (scikit-learn):')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('Confusion matrix from updatable metric (River):')\n",
    "print(metric.cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees for data streams\n",
    "---\n",
    "\n",
    "### `NEWeather` data set\n",
    "\n",
    "**Description:** The National Oceanic and Atmospheric Administration (NOAA),\n",
    "has compiled a database of weather measurements from over 7,000 weather \n",
    "stations worldwide. Records date back to the mid-1900s. Daily measurements\n",
    "include a variety of features (temperature, pressure, wind speed, etc.) as\n",
    "well as a series of indicators for precipitation and other weather-related\n",
    "events. The `NEweather` data set contains data from this database, specifically\n",
    "from the Offutt Air Force Base in Bellevue, Nebraska ranging for over 50 years\n",
    "(1949-1999).\n",
    "\n",
    "**Features:** 8 Daily weather measurements\n",
    " \n",
    "|       Attribute      | Description |\n",
    "|:--------------------:|:-----------------------------|\n",
    "| `temp`                   | Temperature\n",
    "| `dew_pnt`                | Dew Point\n",
    "| `sea_lvl_press`          | Sea Level Pressure\n",
    "| `visibility`             | Visibility\n",
    "| `avg_wind_spd`           | Average Wind Speed\n",
    "| `max_sustained_wind_spd` | Maximum Sustained Wind Speed\n",
    "| `max_temp`               | Maximum Temperature\n",
    "| `min_temp`               | Minimum Temperature\n",
    "\n",
    "\n",
    "**Class:** `rain` | 0: no rain, 1: rain\n",
    " \n",
    "**Number of samples:** 18,159\n",
    "\n",
    "---\n",
    "\n",
    "In this example, we load the data from a csv file with `pandas.read_csv`. \n",
    "\n",
    "Next, we use the `iter_pandas` utility method to iterate over the `DataFrame`. The model in this example is a `HoeffdingTreeClassifier`, a type of decision tree designed for data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from river import tree\n",
    "\n",
    "df = pd.read_csv(\"../datasets/NEweather.csv\")\n",
    "features = df.columns[:-2]\n",
    "data = stream.iter_pandas(X=df[features], y=df['rain'])\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data,\n",
    "                               model=model,\n",
    "                               metric=metric,\n",
    "                               print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tree-based models are popular due to their interpretability.\n",
    "- They use a tree data structure to model the data.\n",
    "- When a sample arrives, it traverses the tree until it reaches a leaf node.\n",
    "- Internal nodes define the path for a data sample based on the values of its features.\n",
    "- Leaf nodes are models that provide predictions for unlabeled-samples and can update their internal state using the labels from labeled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental tree growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "data = stream.iter_pandas(X=df[features], y=df['rain'])\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "model = tree.HoeffdingTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code chunk within the `for` loop in the following cell multiple times we can \"see\" the tree growing. \n",
    "\n",
    "We have simplified this by using `IPython.display` to return multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for i in range(len(df) // 1000 + 1):\n",
    "    chunk = itertools.islice(data, 1000)     # Get chunks of data from the stream\n",
    "    for x, y in chunk:\n",
    "        y_pred = model.predict_one(x)\n",
    "        metric.update(y_true=y, y_pred=y_pred)\n",
    "        model.learn_one(x, y)\n",
    "\n",
    "    print(f'{metric} - n_samples: {metric.cm.n_samples}')\n",
    "    display(model.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "---\n",
    "\n",
    "Pipelines are powerful tools that \"chain\" a sequence of operations. \n",
    "\n",
    "For example, the logistic regression model is sensitive to the scale of the features. Scaling the data so that each feature has mean $\\mu=0$ and variance $\\sigma^2=1$ is generally considered good practice. This is provided by the `StandardScaler` method within the `preprocessing` module of `River`.\n",
    "\n",
    "We can apply the scaling and train the logistic regression sequentially in an elegant manner using a Pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "from river import compose\n",
    "\n",
    "df = pd.read_csv(\"../datasets/NEweather.csv\")\n",
    "features = df.columns[:-2]\n",
    "stream = stream.iter_pandas(X=df[features], y=df['rain'])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "pipeline = compose.Pipeline(\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('log_reg', linear_model.LogisticRegression())\n",
    ")\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=stream,\n",
    "                               model=pipeline,\n",
    "                               metric=metric,\n",
    "                               print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the pipeline by simply calling itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "---\n",
    "\n",
    "### Bike Sharing data set\n",
    "\n",
    "Contains the **hourly and daily count of rental bikes** between the years 2011 and 2012 in the Capital bike-share system with the corresponding weather and seasonal information. Attributes include weather, temperature, date, time, etc.\n",
    "\n",
    "|      Attribute      | Description |\n",
    "|:-------------------:|:-----------------------------------------------------------------|\n",
    "| `instant`             | record index\n",
    "| `dteday`              | date\n",
    "| `season`              | (1:winter, 2:spring, 3:summer, 4:fall)\n",
    "| `yr`                  | year (0: 2011, 1:2012)\n",
    "| `mnth`                | month ( 1 to 12)\n",
    "| `hr`                  | hour (0 to 23)\n",
    "| `holiday`             | weather day is holiday or not\n",
    "| `weekday`             | day of the week\n",
    "| `workingday`          | if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "| `weathersit`          | 1: Clear, Few clouds, Partly cloudy, Partly cloudy<br>2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "| `temp`                | Normalized temperature in Celsius.\n",
    "| `atemp`               | Normalized feeling temperature in Celsius.\n",
    "| `hum`                 | Normalized humidity.\n",
    "| `windspeed`           | Normalized wind speed.\n",
    "| `casual`              | count of casual users\n",
    "| `registered`          | count of registered users\n",
    "\n",
    "**Target:** `cnt` | number of bikes rented including both casual and registered\n",
    " \n",
    "**Number of samples:** 17,389\n",
    "\n",
    "For this example we will use the k-nearest neighbor algorithm.\n",
    "\n",
    "The implementation of KNN Regressor in `River` involves 3 parameters: \n",
    "- `n_neighbors`: Number of neighbors to search for.\n",
    "- `engine`: Search engine used to store the instances and perform search queries. Based on the chosen engines, the search will be exact or approximate. The default engine used for the regressor would be SWINN (Sliding Window-based Nearest Neighbor search using Graphs).\n",
    "- `aggregation_methos`: Method to aggregate the target value of neighbors. Including three methods: `mean`, `median`, and `weighted_mean`.\n",
    "\n",
    "Basically, KNN regressor perform search queries in the data buffer using the defined engine, with predictions obtained by aggregating the values of the closest neighbors stored samples with respect to a query sample.\n",
    "\n",
    "SWINN [(Saolo et al., 2022)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4292758) is a fairly new algorithm. It extends the NNDescend algorithm to handle vertex addition and removal in a FIFO data ingestion policy. The basic idea of SWINN (or NNDescend) is that \"the neighbor of my neighbors might as well be my neighbors\", and the algorithm can be described generally as follows:\n",
    "\n",
    "- A random neighborhood graph is initiated.\n",
    "- For each node in the search graph: refine the current neighborhood by checking if there are better neighborhood options among neighbors of the current neighbors.\n",
    "- If the total number of neighborhood changes is smaller than a given stopping criterion, stop.\n",
    "\n",
    "The algorithm is more effective when the maximum size of the data buffer is greater than $500$. For smaller data sizes, lazy search might be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import neighbors\n",
    "from river.stream import iter_pandas\n",
    "\n",
    "df = pd.read_csv(\"../datasets/bike.csv\")\n",
    "features = df.columns[:-2]\n",
    "\n",
    "data = iter_pandas(X=df[features], y=df['cnt'])\n",
    "model = neighbors.KNNRegressor(n_neighbors=3, aggregation_method=\"weighted_mean\")\n",
    "metric = metrics.R2()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data,\n",
    "                               model=model,\n",
    "                               metric=metric,\n",
    "                               print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbor algorithm assumes that all the features are numeric.\n",
    "\n",
    "In the `bike` data, there are some categorical features. We can easily filter them in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter_pandas(X=df[features], y=df['cnt'])\n",
    "pipeline = (\n",
    "    compose.Discard('season', 'mnth', 'weekday', 'weathersit') |\n",
    "    neighbors.KNNRegressor(n_neighbors=3, aggregation_method=\"weighted_mean\")\n",
    ")\n",
    "metric = metrics.R2()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data,\n",
    "                               model=pipeline,\n",
    "                               metric=metric,\n",
    "                               print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept Drift\n",
    "---\n",
    "\n",
    "Dynamic environments are challenging for machine learning methods because data changes over time.\n",
    "\n",
    "For this example, we will generate a synthetic data stream by concatenating data from 3 different distributions:\n",
    "- $dist_a$: $\\mu=0.8$, $\\sigma=0.05$\n",
    "- $dist_b$: $\\mu=0.4$, $\\sigma=0.02$\n",
    "- $dist_c$: $\\mu=0.6$, $\\sigma=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import plot_data\n",
    "\n",
    "# Generate data for 3 distributions\n",
    "random_state = np.random.RandomState(seed=42)\n",
    "dist_a = random_state.normal(0.8, 0.05, 1000)\n",
    "dist_b = random_state.normal(0.4, 0.02, 1000)\n",
    "dist_c = random_state.normal(0.6, 0.1, 1000)\n",
    "\n",
    "# Concatenate data to simulate a data stream with 2 drifts\n",
    "data = np.concatenate((dist_a, dist_b, dist_c))\n",
    "\n",
    "\n",
    "plot_data(dist_a, dist_b, dist_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As observed above, the synthetic data stream has 3 **abrupt drifts**. An analogy could be trending topics on Twitter which can change quickly.\n",
    "\n",
    "The goal is to detect that drift has occurred, after samples **1000** and **2000** in the synthetic data stream.\n",
    "\n",
    "In this example, we will use the ADaptive WINdowing (`ADWIN`) drift detection method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from river import drift\n",
    "\n",
    "drift_detector = drift.ADWIN()\n",
    "drifts = []\n",
    "\n",
    "for i, val in enumerate(data):\n",
    "    drift_detector.update(val)           # Data is processed one sample at a time\n",
    "    if drift_detector.change_detected:\n",
    "        print(f'Change detected at index {i}')\n",
    "        drifts.append(i)\n",
    "        drift_detector.reset()           # As a best practice, we reset the detector\n",
    "\n",
    "plot_data(dist_a, dist_b, dist_c, drifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Impact on predictive performance\n",
    "\n",
    "Concept drift can negatively impact learning methods if not properly handled. Multiple real-world applications suffer **model degradation** as the models can not adapt to changes in the data.\n",
    "\n",
    "In this example, we will use two popular streaming models:\n",
    "\n",
    "1. The `Hoeffding Tree`\n",
    "2. The `Hoeffding Adaptive Tree`\n",
    "\n",
    "The `Hoeffding Adaptive Tree` uses `ADWIN` to detect changes. If change is detected in a given branch, an alternate branch is created and eventually replaces the original branch if it shows better performance on new data.\n",
    "\n",
    "---\n",
    "### `AGRAWAL` dataset\n",
    "\n",
    "We will load the data from a csv file. The data was generated using the `AGRAWAL` data generator with 3 **gradual drifts** at the 5k, 10k, and 15k marks.\n",
    "\n",
    "The `AGRAWAL` synthetic data generator produces 9 features, 6 numeric and 3 categorical.\n",
    "\n",
    "There are 10 functions for generating binary class labels from the features. These functions determine whether a **loan** should be approved.\n",
    "\n",
    "| Feature    | Description            | Values                                                                |\n",
    "|------------|------------------------|-----------------------------------------------------------------------|\n",
    "| `salary`     | salary                 | uniformly distributed from 20k to 150k                                |\n",
    "| `commission` | commission             | if (salary <   75k) then 0 else uniformly distributed from 10k to 75k |\n",
    "| `age`        | age                    | uniformly distributed from 20 to 80                                   |\n",
    "| `elevel`     | education level        | uniformly chosen from 0 to 4                                          |\n",
    "| `car`        | car maker              | uniformly chosen from 1 to 20                                         |\n",
    "| `zipcode`    | zip code of the town   | uniformly chosen from 0 to 8                                          |\n",
    "| `hvalue`     | value of the house     | uniformly distributed from 50k x zipcode to 100k x zipcode            |\n",
    "| `hyears`     | years house owned      | uniformly distributed from 1 to 30                                    |\n",
    "| `loan`       | total loan amount      | uniformly distributed from 0 to 500k                                  |\n",
    "\n",
    "**Class:** `y` | 0: no loan, 1: loan\n",
    " \n",
    "**Samples:** 20,000\n",
    "\n",
    "`elevel`, `car`, and `zipcode` are categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/agr_a_20k.csv\")\n",
    "features = df.columns[:-2]\n",
    "\n",
    "data = iter_pandas(X=df[features], y=df['class'])\n",
    "model = tree.HoeffdingTreeClassifier(nominal_attributes=['elevel', 'car', 'zipcode'])\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data,\n",
    "                               model=model,\n",
    "                               metric=metric,\n",
    "                               show_memory=True,\n",
    "                               print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/agr_a_20k.csv\")\n",
    "features = df.columns[:-2]\n",
    "\n",
    "data = iter_pandas(X=df[features], y=df['class'])\n",
    "model = tree.HoeffdingAdaptiveTreeClassifier(nominal_attributes=['elevel', 'car', 'zipcode'], seed=42)\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=data,\n",
    "                               model=model,\n",
    "                               metric=metric,\n",
    "                               show_memory=True,\n",
    "                               print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeClassifier\n",
    "from river.stream import iter_pandas\n",
    "from river.metrics import Accuracy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/agr_a_20k.csv\")\n",
    "features = data.columns[:-2]\n",
    "stream = iter_pandas(X=data[features], y=data['class'])\n",
    "model = HoeffdingAdaptiveTreeClassifier(nominal_attributes=['elevel', 'car', 'zipcode'], \n",
    "                                        seed=42)\n",
    "metric = Accuracy()\n",
    "\n",
    "evaluate.progressive_val_score(dataset=stream, \n",
    "                               model=model, \n",
    "                               metric=metric, \n",
    "                               print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    {\"text\":'This is the first document.',\"idd\":1, \"cluster\": 1, \"cluster\":1},\n",
    "    {\"text\":'This document is the second document.',\"idd\":2,\"cluster\": 1},\n",
    "    {\"text\":'And this is super unrelated.',\"idd\":3,\"cluster\": 2},\n",
    "    {\"text\":'Is this the first document?',\"idd\":4,\"cluster\": 1},\n",
    "    {\"text\":'This is super unrelated as well',\"idd\":5,\"cluster\": 2},\n",
    "    {\"text\":'Test text',\"idd\":6,\"cluster\": 5}\n",
    "]\n",
    "\n",
    "stopwords = ['stop', 'the', 'to', 'and', 'a', 'in', 'it', 'is', 'I']\n",
    "\n",
    "metric = metrics.AdjustedRand()\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    feature_extraction.BagOfWords(lowercase=True, ngram_range=(1, 2), stop_words=stopwords),\n",
    "    cluster.TextClust(real_time_fading=False, fading_factor=0.001, tgap=100, auto_r=True,\n",
    "    radius=0.9)\n",
    ")\n",
    "\n",
    "for x in corpus:\n",
    "    y_pred = model.predict_one(x[\"text\"])\n",
    "    y = x[\"cluster\"]\n",
    "    metric = metric.update(y,y_pred)\n",
    "    model = model.learn_one(x[\"text\"])\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-river\n",
    "## Multi target regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MicroAverage(MAE): 28.36\n"
     ]
    }
   ],
   "source": [
    "from river import evaluate, compose\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from river import stream\n",
    "from sklearn import datasets\n",
    "from torch import nn\n",
    "from deep_river.regression.multioutput import MultiTargetRegressor\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.dense0 = nn.Linear(n_features, 3)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.dense0(X)\n",
    "        return X\n",
    "\n",
    "dataset = stream.iter_sklearn_dataset(\n",
    "    dataset=datasets.load_linnerud(),\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    MultiTargetRegressor(\n",
    "        module=MyModule,\n",
    "        loss_fn='mse',\n",
    "        lr=0.3,\n",
    "        optimizer_fn='sgd',\n",
    "    )\n",
    ")\n",
    "\n",
    "metric = metrics.multioutput.MicroAverage(metrics.MAE())\n",
    "ev = evaluate.progressive_val_score(dataset, model, metric)\n",
    "print(f\"MicroAverage(MAE): {metric.get():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6728\n",
      "Time conducted: 0.33247828483581543 seconds.\n"
     ]
    }
   ],
   "source": [
    "from river import metrics, datasets, preprocessing, compose\n",
    "from deep_river import classification\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import manual_seed\n",
    "import time\n",
    "\n",
    "_ = manual_seed(42)\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.dense0 = nn.Linear(n_features, 5)\n",
    "        self.nonlin = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(5, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(X)\n",
    "        return X\n",
    "\n",
    "model_pipeline = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    classification.Classifier(module=MyModule, loss_fn='binary_cross_entropy', optimizer_fn='adam')\n",
    ")\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "time_start = time.time()\n",
    "for x, y in dataset:\n",
    "    y_pred = model_pipeline.predict_one(x)  # make a prediction\n",
    "    metric = metric.update(y, y_pred)  # update the metric\n",
    "    model_pipeline = model_pipeline.learn_one(x, y)  # make the model learn\n",
    "time_end = time.time()\n",
    "\n",
    "print(f\"Accuracy: {metric.get():.4f}\")\n",
    "print(f\"Time conducted: {time_end-time_start} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 8] nodename nor servname provided, or not known>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/urllib/request.py:1346\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1346\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/http/client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/http/client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/http/client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m \n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/http/client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/http/client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1447\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/http/client.py:946\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/socket.py:823\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    822\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 823\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    824\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    953\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m Pipeline(scaler, ae)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m     31\u001b[0m     score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore_one(x)\n\u001b[1;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlearn_one(x\u001b[38;5;241m=\u001b[39mx)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/site-packages/river/datasets/base.py:325\u001b[0m, in \u001b[0;36mRemoteDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_downloaded:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_downloaded:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething went wrong during the download\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/site-packages/river/datasets/base.py:272\u001b[0m, in \u001b[0;36mRemoteDataset.download\u001b[0;34m(self, force, verbose)\u001b[0m\n\u001b[1;32m    269\u001b[0m directory\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    270\u001b[0m archive_path \u001b[38;5;241m=\u001b[39m directory\u001b[38;5;241m.\u001b[39mjoinpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Notify the user\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    275\u001b[0m         meta \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/urllib/request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    516\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 517\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    520\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/urllib/request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    533\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 534\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    535\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/urllib/request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/river_extra/lib/python3.9/urllib/request.py:1349\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1347\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1350\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 8] nodename nor servname provided, or not known>"
     ]
    }
   ],
   "source": [
    "from deep_river.anomaly import Autoencoder\n",
    "from river import metrics\n",
    "from river.datasets import CreditCard\n",
    "from torch import nn\n",
    "import math\n",
    "from river.compose import Pipeline\n",
    "from river.preprocessing import MinMaxScaler\n",
    "\n",
    "dataset = CreditCard().take(5000)\n",
    "metric = metrics.ROCAUC(n_thresholds=50)\n",
    "\n",
    "class MyAutoEncoder(nn.Module):\n",
    "    def __init__(self, n_features, latent_dim=3):\n",
    "        super(MyAutoEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_features, latent_dim)\n",
    "        self.nonlin = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(latent_dim, n_features)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.linear1(X)\n",
    "        X = self.nonlin(X)\n",
    "        X = self.linear2(X)\n",
    "        return self.sigmoid(X)\n",
    "\n",
    "ae = Autoencoder(module=MyAutoEncoder, lr=0.005)\n",
    "scaler = MinMaxScaler()\n",
    "model = Pipeline(scaler, ae)\n",
    "\n",
    "for x, y in dataset:\n",
    "    score = model.score_one(x)\n",
    "    model = model.learn_one(x=x)\n",
    "    metric = metric.update(y, score)\n",
    "\n",
    "print(f\"ROCAUC: {metric.get():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "river_extra",
   "language": "python",
   "name": "river_extra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
