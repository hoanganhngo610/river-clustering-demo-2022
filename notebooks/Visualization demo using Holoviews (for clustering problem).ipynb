{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced model performance visualization with `Holoviews` (for clustering problems)\n",
    "\n",
    "This notebook demonstrates how to perform a real-time visualization for clustering problems with `Holoviews`. The requirement for this notebook to run includes the installation of the following libraries:\n",
    "\n",
    "* `holoviews`: HoloViews is an open-source Python library designed to make data analysis and visualization seamless and simple.\n",
    "* `panel`: Panel is an open-source Python library that lets you create custom interactive web apps and dashboards by connecting user-defined widgets to plots, images, tables, or text.\n",
    "* `streamz`: Streamz helps you build pipelines to manage continuous streams of data. It is simple to use in simple cases, but also supports complex pipelines that involve branching, joining, flow control, feedback, back pressure, and so on.\n",
    "\n",
    "Moreover, apart from `river`, `river-extra` should also be installed. This package contains additional estimators that have not been put into the main `river` package. Currently, it holds various useful metrics for clustering problems in the `metrics.cluster` submodule, although not as commonly used as those already available in the main package.\n",
    "\n",
    "These libraries are very easy to install via `pip`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with using classification metrics for clustering problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle problem regarding the use of classification metrics for clustering problems lie in the fact that these metrics cannot take into the fact that the order of groups/clusters area commutable. We consider the following mainly used metric, `F1`, as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "y_true = [0, 0, 1, 1, 2, 2]\n",
    "y_pred = [1, 1, 2, 2, 0, 0]\n",
    "\n",
    "metric = metrics.FBeta(beta=1) # When `beta` equals 1, this metrics is equivalent to the F1 score.\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    metric = metric.update(yt, yp)\n",
    "    \n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of F1 score for this clustering result returns in a score of $0%$, which totally does not make sense since the predicted result is only a permutation of the actual labels. As such, it is extremely important to implement and take intro consideration/use the class of clustering-specific metrics. In `River`, we have implemented the largest family of these metrics in any currently available Python packages for conventional/online machine learning, in a fully incremental fashion.\n",
    "\n",
    "The list of implemented metrics includes (but is not limited to):\n",
    "\n",
    "- **EXTERNAL CLUSTERING METRICS**: These metrics requires the existence of external data, i.e the ground truth or true labels. In other words, these metrics represent the correlation, or agreement, between the predicted and actual results. Usually speaking, these metrics will lie within the range $[0, 1]$, and the higher the metric, the better the clustering result is.\n",
    "\n",
    "    This class of metrics include the following:\n",
    "    \n",
    "    - Fowlkes-Mallows Index\n",
    "    - Jaccard Index\n",
    "    - Matthew's Correlation Coefficient\n",
    "    - Prevalence Threshold\n",
    "    - Purity\n",
    "    - Q0 and Q2 Indices\n",
    "    - (Adjusted) Rand Index\n",
    "    - Variation of Information\n",
    "    - VBeta (including Completeness, Homogeneity and Completeness)\n",
    "\n",
    "\n",
    "- **INTERNAL CLUSTERING METRICS**: As the name suggests, this class of metrics only uses the internal information generated from the clustering problem, including cluster centers' positions, predicted labels, distances from the new data point to cluster centers, etc. without the need of any external information. These metrics do not have a definitive range; however, normally, the lower these metrics are, the better the clustering result is.\n",
    "\n",
    "    This class of metrics include the following:\n",
    "\n",
    "    - Bayesian Information Criterion (BIC)\n",
    "    - Davies Bouldin (DB) Index\n",
    "    - Generalized Dunn's indices (GD43 and GD53)\n",
    "    - (Root) Mean Squared Standard Deviation\n",
    "    - SD Validity Index\n",
    "    - Separation\n",
    "    - Silhouette\n",
    "    - Sum of Squares Between Clusters (including Calinski Harabasz (CH) Index, Hartigan Index (H-Index) and WB Index)\n",
    "    - Sum of Squares Within Clusters\n",
    "    - Xie-Beni index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will consider another example to see the difference between a classification-specific metric and a clustering-specific metric, i.e in this case Cohen-Kappa index and VBeta index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "\n",
    "y_true = [0, 0, 1, 1, 2, 2]\n",
    "y_pred = [1, 1, 2, 2, 0, 0]\n",
    "\n",
    "kappa = metrics.CohenKappa()\n",
    "vbeta = metrics.VBeta(beta=1.0)\n",
    "\n",
    "for yt, yp in zip(y_true, y_pred):\n",
    "    kappa = kappa.update(yt, yp)\n",
    "    vbeta = vbeta.update(yt, yp)\n",
    "    \n",
    "print(kappa)\n",
    "print(vbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can easily see that VBeta does a better job in capturing the agreement between `y_true` and `y_pred`, while CohenKappa returns a negative value without any interpretation value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will extensively visualize clustering algorithms using `Holoviews`. Within River, we have implemented a total of 6 (5 camera-ready, 1 pending) clustering algorithms of different methods, including:\n",
    "\n",
    "- CluStream\n",
    "- DenStream\n",
    "- DBStream\n",
    "- Incremental KMeans\n",
    "- STREAMKMeans\n",
    "- EvoStream (a fairly new clustering method, developed based on evolutionary algorithms)\n",
    "\n",
    "River is also the package that includes the most number of clustering algorithms on a unified framework, until this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "import river\n",
    "from river import cluster\n",
    "from river.metrics.report import ClassificationReport\n",
    "from river.tree import HoeffdingTreeClassifier, \\\n",
    "                       HoeffdingAdaptiveTreeClassifier,  \\\n",
    "                       SGTClassifier, \\\n",
    "                       ExtremelyFastDecisionTreeClassifier\n",
    "from river.stream import iter_pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import panel as pn\n",
    "\n",
    "import streamz\n",
    "import streamz.dataframe\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews.streams import Buffer\n",
    "from holoviews import opts\n",
    "\n",
    "import copy\n",
    "\n",
    "hv.extension('bokeh')\n",
    "\n",
    "opts.defaults(opts.Curve(width=900, height=350, show_grid=True, tools=['hover'], framewise=True))\n",
    "opts.defaults(opts.Table(width=895, height=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')\n",
    "\n",
    "window_size = 1000\n",
    "\n",
    "metrics = [river.metrics.MutualInfo(), river.metrics.FowlkesMallows()]\n",
    "rolling_metrics = [river.utils.Rolling(metric, window_size=window_size) \n",
    "                   for metric in metrics]\n",
    "metric_names = [metric.__class__.__name__ for metric in metrics]\n",
    "\n",
    "TrackedModel = namedtuple('TrackedModel', ['model', 'rolling_metrics', 'metrics'])\n",
    "\n",
    "tracked_models = [\n",
    "    TrackedModel(cluster.DenStream(decaying_factor=0.01, beta=0.5, mu=2.5, epsilon=0.5, n_samples_init=10),\n",
    "                 copy.deepcopy(rolling_metrics), copy.deepcopy(metrics)),\n",
    "    TrackedModel(cluster.DBSTREAM(clustering_threshold=1.5, fading_factor=0.05, cleanup_interval=10,\n",
    "                                  intersection_factor=0.5, minimum_weight=1),\n",
    "                 copy.deepcopy(rolling_metrics), copy.deepcopy(metrics))\n",
    "]\n",
    "n_models = len(tracked_models)\n",
    "\n",
    "model_names = [item.model.__class__.__name__ for item in tracked_models]\n",
    "\n",
    "# metrics = ['acc', 'kappa']\n",
    "\n",
    "# This section creates the streaming dataframe\n",
    "df = pd.DataFrame([], columns=['model', 'metric', 'sample', 'current', 'mean']).set_index('sample')\n",
    "streaming_df = streamz.dataframe.DataFrame(streamz.Stream(), example=df)\n",
    "\n",
    "# Create dictionary to store plot items\n",
    "metric_dict = defaultdict(list)\n",
    "\n",
    "PlotItem = namedtuple('PlotItem', ['model_id', 'curve_1', 'curve_2'])\n",
    "\n",
    "# This section creates the DynamicMap objects (curves and tables)\n",
    "for model_name, metric_name in itertools.product(model_names, metric_names):\n",
    "        # The following line creates a DynamicMap for each metric - model combination\n",
    "        # (e.g. accuracy_mean, kappa_mean, etc.)\n",
    "        item = PlotItem(\n",
    "            model_id=model_name,\n",
    "            curve_1=hv.DynamicMap(hv.Curve, streams=[\n",
    "                Buffer(streaming_df[(streaming_df['model']==model_name) & (streaming_df['metric']==metric_name)]['current'])]),\n",
    "            curve_2=hv.DynamicMap(hv.Curve, streams=[\n",
    "                Buffer(streaming_df[(streaming_df['model']==model_name) & (streaming_df['metric']==metric_name)]['mean'])])\n",
    "        )\n",
    "        item.curve_2.opts(opts.Curve(line_dash='dashed'))\n",
    "\n",
    "        metric_dict[metric_name].append(item)\n",
    "\n",
    "# This section is sorting the overlays \n",
    "# (overlaying accuracy_mean with accuracy_current etc.) and \n",
    "# grouping the DynamicMaps with their respective tables \n",
    "# into appropriate panel tabs.\n",
    "\n",
    "# Default variables.\n",
    "layout_tabs = pn.Tabs()\n",
    "\n",
    "for metric, metric_elements in metric_dict.items():\n",
    "    curves = []\n",
    "    for i, item in enumerate(metric_elements):\n",
    "        curves.append(item.curve_1.relabel(f'current_{i}'))\n",
    "        curves.append(item.curve_2.relabel(f'mean_{i}'))\n",
    "    table=hv.DynamicMap(hv.Table, streams=[\n",
    "        Buffer(streaming_df[streaming_df['metric']==metric][['model', 'current', 'mean']], length=n_models)\n",
    "    ])\n",
    "    overlayed_dmap = reduce((lambda x, y: x * y), curves)\n",
    "    overlayed_dmap.opts(legend_position='right', legend_offset=(20, 0), xlabel='sample', ylabel='score')\n",
    "    overlayed_dmap.opts(hv.opts.Curve(color=hv.Cycle('Category20'), bgcolor='#fafafa'))\n",
    "    overlayed_dmap = pn.pane.HoloViews(overlayed_dmap, linked_axes=False)\n",
    "    dmap_layout = pn.Column(overlayed_dmap, table)\n",
    "    layout_tabs.append((metric, dmap_layout))      \n",
    "\n",
    "# This variable is the layout of the tabs of metrics.\n",
    "layout_tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dfstream, models):\n",
    "    n_wait = 100\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_csv(\"../datasets/agr_a_20k.csv\")\n",
    "    features = data.columns[:-2]\n",
    "    stream = iter_pandas(X=data[features], y=data['class'])\n",
    "    \n",
    "\n",
    "    for sample_cnt, (x, y_true) in enumerate(stream):\n",
    "        for component in models:\n",
    "            y_pred = component.model.predict_one(x)\n",
    "            for metric in component.metrics:\n",
    "                metric.update(y_true, y_pred)\n",
    "            for rolling_metric in component.rolling_metrics:\n",
    "                rolling_metric.update(y_true, y_pred)\n",
    "            component.model.learn_one(x, y_true)\n",
    "        \n",
    "        if (sample_cnt + 1) % n_wait == 0:\n",
    "            results = []\n",
    "            for component, metric_name in itertools.product(models, metric_names):\n",
    "                model_id = component.model.__class__.__name__\n",
    "                if metric_name == 'MutualInfo':\n",
    "                    mean_score = component.metrics[0].get()\n",
    "                    curr_score = component.rolling_metrics[0].get()\n",
    "                elif metric_name == 'FowlkesMallows':\n",
    "                    mean_score = component.metrics[1].get()\n",
    "                    curr_score = component.rolling_metrics[1].get()\n",
    "                results.append((model_id, metric_name, sample_cnt + 1, curr_score, mean_score))\n",
    "            dfstream.emit(\n",
    "                pd.DataFrame(results,\n",
    "                             columns=['model', 'metric', 'sample', 'current', 'mean']).set_index('sample'))\n",
    "            \n",
    "\n",
    "    # Reset DF\n",
    "    df_metrics = pd.DataFrame(index=[], columns=['model', 'metric', 'sample', 'current', 'mean']).set_index('sample')\n",
    "    streaming_df = streamz.dataframe.DataFrame(streamz.Stream(), example=df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(streaming_df, tracked_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "river_extra",
   "language": "python",
   "name": "river_extra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
